{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = 'https://www.biomart.cn/'\n",
    "max_retries = 5\n",
    "\n",
    "# 重试间隔时间（秒）\n",
    "retry_interval = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method(url3,title3,dir3):\n",
    "    full_url = root_url + url3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 发送请求\n",
    "            response4 = requests.get(full_url, timeout=5)  # 设置超时时间为 5 秒\n",
    "            response4.raise_for_status()  # 如果响应状态码不是 200，抛出异常\n",
    "            soup = BeautifulSoup(response4.text, 'html.parser')\n",
    "            break  # 请求成功，退出循环\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                print(\"Max retries reached. Giving up.\")\n",
    "    #divs_with_ids  = soup.find_all('div', id=True)\n",
    "    divs_with_contents  = soup.find_all('div', class_='rich-box')\n",
    "    divs_with_ids = soup.find_all('div', class_='tw-flex tw-items-center tw-my-20')\n",
    "    #print (len(divs_with_contents),len(divs_with_ids))\n",
    "    result_id = []\n",
    "    result_content = []\n",
    "    for div in divs_with_ids:\n",
    "        #div_id = div['id']\n",
    "        #if div_id in ['__next','main-box','right-container','j-dxy-bottom']:continue\n",
    "        paragraphs = div.find('p').get_text()\n",
    "        paragraphs = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', paragraphs).strip()\n",
    "        result_id.append({\n",
    "            'id': paragraphs\n",
    "        })\n",
    "    for div in divs_with_contents:\n",
    "        div_html = str(div)\n",
    "        markdown_content =  md(div_html)\n",
    "        result_content.append({\n",
    "            'markdown': markdown_content\n",
    "        })\n",
    "\n",
    "## 转markdown\n",
    "    outfile_name = f\"{dir3}/{title3}\"\n",
    "    if os.path.exists(f\"{outfile_name}.md\"):\n",
    "        outfile_name = outfile_name + \"_add\"\n",
    "    with open(f\"{outfile_name}.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(0,len(result_content)):\n",
    "            #f.write(f\"## {item['id']}\\n\\n\")\n",
    "            f.write(f\"## {result_id[i]['id']}\\n\\n\")\n",
    "            f.write(result_content[i]['markdown'])\n",
    "            f.write(\"\\n\\n---\\n\\n\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_level2(url2,dir2):\n",
    "    full_url = root_url + url2\n",
    "    #print (full_url)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 发送请求\n",
    "            response3 = requests.get(full_url, timeout=5)  # 设置超时时间为 5 秒\n",
    "            response3.raise_for_status()  # 如果响应状态码不是 200，抛出异常\n",
    "            soup = BeautifulSoup(response3.text, 'html.parser')\n",
    "            break  # 请求成功，退出循环\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                print(\"Max retries reached. Giving up.\")\n",
    "    soup = BeautifulSoup(response3.text, 'html.parser')\n",
    "    ## 找到Methods\n",
    "    links  = soup.find_all('a', class_='tw-mb-10 tw-inline-block tw-w-full tw-cursor-pointer tw-rounded-8 tw-bg-other-400 tw-p-20 last:tw-mb-none hover:tw-bg-other-300 hover:tw-text-current md:tw-mb-20')\n",
    "    for link in links:\n",
    "        method_title = link.find('b').get_text()\n",
    "        method_title = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', method_title).strip()\n",
    "        href = link.get('href')\n",
    "        #text = link.get_text()\n",
    "        if href:\n",
    "            print(f'{href}' + '\\t' + f'{method_title}')\n",
    "            get_method(href,method_title,dir2)\n",
    "        else:\n",
    "            #pass\n",
    "            print ('not found')\n",
    "        time.sleep(1)\n",
    "        #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dir = pd.read_csv('/public/home/liujunwu/workdir/MEPC/ExperimentMethod/empty.dir',header=0)\n",
    "empty_dir.columns = ['dir']\n",
    "log_raw = pd.read_csv('/public/home/liujunwu/workdir/MEPC/ExperimentMethod/log_all.bak',header=None,sep='\\t',on_bad_lines='skip')\n",
    "log_raw.columns = ['url','base_dir']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dir['base_dir'] = empty_dir['dir'].apply(lambda x: x.split('/')[-1])\n",
    "print (empty_dir.shape)\n",
    "empty_dir.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = pd.merge(empty_dir,log_raw,how='left',left_on='base_dir',right_on='base_dir')\n",
    "print (merged_dir.shape)\n",
    "merged_dir.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir[merged_dir['url'].str.match('method', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = merged_dir[merged_dir['url'].notnull()]\n",
    "merged_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in merged_dir.iterrows():\n",
    "    print (row['url'],row['dir'])\n",
    "    exp_level2(row['url'],row['dir'])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_level2('/lab-web/exp/316nrkggo40di/31o6i0ego4hki.html','/public/home/liujunwu/workdir/MEPC/ExperimentMethod/植物学实验/植物染色体核型分析')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
