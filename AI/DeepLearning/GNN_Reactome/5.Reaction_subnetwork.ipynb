{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5406bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.nn import GCNConv,GATConv,SAGEConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch.nn import Linear\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb887cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理边关系\n",
    "import json\n",
    "edges_file = \"/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/reactome_reaction.edges.txt\"\n",
    "edges = pd.read_csv(edges_file,header=None,sep=\"\\t\")\n",
    "edges.columns = [\"edge1\",\"edge2\",\"edge1_type\",\"edge2_type\"] \n",
    "uniq_output_nodes = sorted(list(set(edges[\"edge2\"].tolist()))) ## 取得唯一入度的点\n",
    "uniq_nodes = sorted(list(set(edges[\"edge1\"].tolist()) | set(edges[\"edge2\"].tolist())))\n",
    "nodes_index_dict = dict(map(reversed, enumerate(uniq_nodes))) ## 将ID和对应的index存为字典\n",
    "#with open(\"/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/reactome_reaction.uniqnodes.json\", \"w\", encoding='utf-8') as f:\n",
    "    # json.dump(dict_, f)  # 写为一行\n",
    "    #json.dump(nodes_index_dict, f, indent=2, sort_keys=True, ensure_ascii=False)  # 写为多行\n",
    "# index_nodes_dict = dict(enumerate(uniq_nodes))  ## 将index和对应的ID存为字典\n",
    "print (len(uniq_nodes))\n",
    "print (nodes_index_dict[\"R-HSA-5340130\"])\n",
    "edges[\"edge1\"] = edges[\"edge1\"].map(nodes_index_dict)\n",
    "edges[\"edge2\"] = edges[\"edge2\"].map(nodes_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(uniq_output_nodes))\n",
    "print (edges.iloc[0:4,0:4])\n",
    "print (edges.loc[0,\"edge1\"])\n",
    "sub_node_edges = []\n",
    "for index, node in enumerate(uniq_output_nodes):  \n",
    "    sub_node = edges[edges[\"edge2\"] == nodes_index_dict[node]]\n",
    "    for e in range(sub_node.shape[0]):\n",
    "        sub_node_edges.append([sub_node.iat[e,0],sub_node.iat[e,1]])\n",
    "edges_torch = torch.tensor(sub_node_edges,dtype = torch.int64).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3daacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_torch[1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (edges_torch.shape)\n",
    "torch.save(edges_torch,\"/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/Reactome_reaction_edges.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e739669",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取reaction包含的基因\n",
    "ReactionGeneFile = \"/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/reactome_reaction.ProteinReactionNodes.txt\"\n",
    "ReactionGene = pd.read_csv(ReactionGeneFile,header=None,sep='\\t')\n",
    "ReactionGene.columns = [\"Reaction\",\"Gene\"]\n",
    "ReactionGene_dict = dict(zip(ReactionGene[\"Reaction\"],ReactionGene[\"Gene\"])) ## Reaction 对应的Gene\n",
    "print (ReactionGene_dict['R-HSA-1112666'])\n",
    "del ReactionGene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTEx_expfile = \"/public/home/liujunwu/workdir/scripts/GNN_Reactome/GTEx_exp/GTEx_exp.csv\" ## 读取GTEx表达量文件\n",
    "#GTEx_expfile = \"/public/home/liujunwu/workdir/scripts/GNN_Reactome/GTEx_exp/GTEx_exp_down5000.csv\" ## 小数据集测试\n",
    "\n",
    "GTEx_exp = pd.read_csv(GTEx_expfile,header=0,sep=\",\")\n",
    "GTEx_genelist = GTEx_exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTEx_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GTEx的样本标签\n",
    "label_file = \"/public/home/liujunwu/workdir/scripts/GNN_Reactome/GTEx_exp/GSEA/GTEx.sample_label.txt\"\n",
    "sample_labels = pd.read_csv(label_file,header=0,sep=\"\\t\")\n",
    "sample_full_labels = sorted(list(set(sample_labels[\"SMTS\"].tolist()))) ## 唯一值\n",
    "sample_full_labels_dict = dict(zip(sample_full_labels,range(len(sample_full_labels)))) ## label对应的index\n",
    "list_labels = []\n",
    "print (sample_full_labels_dict)\n",
    "sample_dict = dict(zip(sample_labels[\"SAMPID\"],sample_labels[\"SMTS\"]))\n",
    "print (sample_full_labels_dict[sample_dict[\"GTEX-1GF9W-1326-SM-7P8PX\"]])\n",
    "#for x in range(len(sample_full_labels)):\n",
    "#    list_labels.append([x])\n",
    "#list_labels_tensor = torch.tensor(list_labels,dtype=torch.int64)\n",
    "#print (list_labels_tensor[sample_full_labels_dict[sample_dict[\"GTEX-1GF9W-1326-SM-7P8PX\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(n_components=2)\n",
    "print (GTEx_exp.loc[0:5,[\"Sample\",\"Tissue\",\"WASH7P\"]])\n",
    "x = GTEx_exp.iloc[:,2:10].values\n",
    "#x = StandardScaler().fit_transform(x)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "print (type(principalComponents))\n",
    "print (principalComponents)\n",
    "#principalDf = pd.DataFrame(data = principalComponents,columns = [\"PCA1\",\"PCA2\"])\n",
    "#print (principalDf.iloc[0:3,0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = pd.DataFrame(np.zeros((4,1)),columns=[\"PCA1\"])\n",
    "sample_reaction_pca = pd.DataFrame(size=(GTEx_exp.shape[0],0))\n",
    "print (sample_reaction_pca)\n",
    "#print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对于每一个reaction , PCA 得到的主成分 PCA1值\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(n_components=2)\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "sample_reaction_pca = pd.DataFrame()\n",
    "n = 0\n",
    "reaction_PCA_values = []\n",
    "for node in iter(uniq_nodes): ## 计算所有reaction节点 对应的PCA1 values\n",
    "    #print (node)\n",
    "    #node = \"R-HSA-1006169\"\n",
    "    if (node in ReactionGene_dict.keys()):\n",
    "        genelist = ReactionGene_dict[node].split(\",\")\n",
    "        common_gene = list(set(genelist) & set(GTEx_genelist))\n",
    "        ## 去除bulk中不存在的基因\n",
    "        sub_exp = GTEx_exp.loc[:,common_gene]\n",
    "        #print (sub_exp)\n",
    "        if (sub_exp.shape[1] >= 2):\n",
    "            #sub_exp = sub_exp.values\n",
    "            #sub_exp = StandardScaler().fit_transform(sub_exp.values)\n",
    "            principalComponents = pca.fit_transform(sub_exp.values)\n",
    "            PCA_values =  pd.DataFrame(data = principalComponents,columns = [\"PCA1\",\"PCA2\"])\n",
    "            node_PCA_values = PCA_values[\"PCA1\"]\n",
    "        elif (sub_exp.shape[1] == 1): ## 如果reaction中只包含一个基因，无法做PCA，则对该基因表达量进行标准化\n",
    "            sub_exp = StandardScaler().fit_transform(sub_exp.values)\n",
    "            node_PCA_values = pd.DataFrame(data = sub_exp, columns=[\"PCA1\"])\n",
    "            #print (node_PCA_values)\n",
    "        else:\n",
    "            node_PCA_values = pd.DataFrame(np.zeros((GTEx_exp.shape[0],1)),columns=[\"PCA1\"])\n",
    "    else:\n",
    "        node_PCA_values = pd.DataFrame(np.zeros((GTEx_exp.shape[0],1)),columns=[\"PCA1\"])\n",
    "    reaction_PCA_values.append(node_PCA_values)\n",
    "    #sample_reaction_pca = pd.concat([sample_reaction_pca,node_PCA_values],axis=1,join='outer',ignore_index=True)\n",
    "    #n += 1\n",
    "    #if n >3:break\n",
    "    #break\n",
    "sample_reaction_pca = pd.concat(reaction_PCA_values,axis=1,join='outer',ignore_index=True)\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "print (sample_reaction_pca.iloc[0:10,0:10])\n",
    "print (sample_reaction_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_reaction_pca.loc[11714,].values\n",
    "b = a.reshape(1,a.shape[0])\n",
    "print (b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "GTEx_dataset = []\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "for sample in GTEx_exp[\"Sample\"]:\n",
    "    #print (sample,n)\n",
    "    sampleValue = sample_reaction_pca.loc[n,].values\n",
    "    sampleValue = sampleValue.reshape(sampleValue.shape[0],1)\n",
    "    sample_node_feature = Data(x=torch.tensor(sampleValue ,dtype = torch.float32),y=torch.tensor(sample_full_labels_dict[sample_dict[sample]]),edge_index = edges_torch)\n",
    "    GTEx_dataset.append(sample_node_feature)\n",
    "    n = n+1\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "print (len(GTEx_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (GTEx_dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ca60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (list_labels_tensor[sample_full_labels_dict[sample_dict[\"GTEX-1GF9W-1326-SM-7P8PX\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10525ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (GTEx_dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6037af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features,hidden_channels,num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels,num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch,edge_weight=None):\n",
    "        # 1. 获得节点嵌入\n",
    "        x = self.conv1(x, edge_index,edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index,edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index,edge_weight)\n",
    "        \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)   # [batch_size, hidden_channels]\n",
    "        \n",
    "        # 3. 分类器\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "?GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(num_node_features=1,hidden_channels=64,num_classes=31).to(device)\n",
    "#model = NeuralNetwork().to(device)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6443996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "torch.set_num_threads(48)\n",
    "#print (train_dataset[0])\n",
    "train_dataset1 = GTEx_dataset[0:15000]\n",
    "print (train_dataset1[20])\n",
    "train_loader = DataLoader(train_dataset1, batch_size=32,shuffle=True)\n",
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        #print (data.y)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index,data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:                            # 批遍历测试集数据集。\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch) # 一次前向传播\n",
    "        pred = out.argmax(dim=1)   # 使用概率最高的类别\n",
    "        #print (pred,data.y)\n",
    "        correct += int((pred == data.y).sum())           # 检查真实标签\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "model.train()\n",
    "print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    train()\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'))\n",
    "    train_acc = test(train_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_dataset1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters(): print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "model.eval()\n",
    "test_data = GTEx_dataset[15000:]\n",
    "print (len(test_data))\n",
    "test_loader = DataLoader(test_data, batch_size=100,shuffle=True)\n",
    "test_acc = test(test_loader)\n",
    "#print (test_acc.)\n",
    "print(f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e298058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "saved_dict = {\n",
    " 'model': model.state_dict(),\n",
    " 'opt': optimizer.state_dict()\n",
    "}\n",
    "torch.save(saved_dict, '/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/reaction.PCA.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46171543",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/public/home/liujunwu/workdir/scripts/GNN_Reactome/reaction_file/reaction.PCA_convEdgeWeight.1018.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
