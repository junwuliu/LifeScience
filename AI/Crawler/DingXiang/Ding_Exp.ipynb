{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = 'https://www.biomart.cn/'\n",
    "output_dir = '/public/home/liujunwu/workdir/MEPC/ExperimentMethod'\n",
    "# 重试间隔和时间（秒）\n",
    "max_retries = 5\n",
    "retry_interval = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_request(url):\n",
    "    response = requests.get(url)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 发送请求\n",
    "            response = requests.get(url, timeout=3)  # 设置超时时间为 5 秒\n",
    "            response.raise_for_status()  # 如果响应状态码不是 200，抛出异常\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            break  # 请求成功，退出循环\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_interval)\n",
    "            else:\n",
    "                print(\"Max retries reached. Giving up.\")\n",
    "                soup = None\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method(url3,title3,dir3):\n",
    "    full_url = root_url + url3\n",
    "    soup = try_request(full_url)\n",
    "    #divs_with_ids  = soup.find_all('div', id=True)\n",
    "    divs_with_contents  = soup.find_all('div', class_='rich-box')\n",
    "    divs_with_ids = soup.find_all('div', class_='tw-flex tw-items-center tw-my-20')\n",
    "    #print (len(divs_with_contents),len(divs_with_ids))\n",
    "    result_id = []\n",
    "    result_content = []\n",
    "    for div in divs_with_ids:\n",
    "        #div_id = div['id']\n",
    "        #if div_id in ['__next','main-box','right-container','j-dxy-bottom']:continue\n",
    "        paragraphs = div.find('p').get_text()\n",
    "        paragraphs = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', paragraphs).strip()\n",
    "        result_id.append({\n",
    "            'id': paragraphs\n",
    "        })\n",
    "    for div in divs_with_contents:\n",
    "        div_html = str(div)\n",
    "        markdown_content =  md(div_html)\n",
    "        result_content.append({\n",
    "            'markdown': markdown_content\n",
    "        })\n",
    "\n",
    "## 转markdown\n",
    "    outfile_name = f\"{dir3}/{title3}\"\n",
    "    with open(f\"{outfile_name}.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(0,len(result_content)):\n",
    "            #f.write(f\"## {item['id']}\\n\\n\")\n",
    "            f.write(f\"## {result_id[i]['id']}\\n\\n\")\n",
    "            f.write(result_content[i]['markdown'])\n",
    "            f.write(\"\\n\\n---\\n\\n\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_method('/lab-web/method/34ndj3ogo2e00.html','test',output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_level2(url2,dir2):\n",
    "    full_url = root_url + url2\n",
    "    #print (full_url)\n",
    "    soup = try_request(full_url)\n",
    "    ## 找到Methods\n",
    "    links  = soup.find_all('a', class_='tw-mb-10 tw-inline-block tw-w-full tw-cursor-pointer tw-rounded-8 tw-bg-other-400 tw-p-20 last:tw-mb-none hover:tw-bg-other-300 hover:tw-text-current md:tw-mb-20')\n",
    "    for link in links:\n",
    "        method_title = link.find('b').get_text()\n",
    "        method_title = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', method_title).strip()\n",
    "        href = link.get('href')\n",
    "        #text = link.get_text()\n",
    "        if href:\n",
    "            print(f'{href}' + '\\t' + f'{method_title}')\n",
    "            get_method(href,method_title,dir2)\n",
    "        else:\n",
    "            #pass\n",
    "            print ('not found')\n",
    "        time.sleep(1)\n",
    "        #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_level1(url1,dir1):\n",
    "    full_url = root_url + url1\n",
    "    soup = try_request(full_url)\n",
    "    total_page = soup.find_all('li', title=True)\n",
    "    max_page_id = 1\n",
    "    for page in total_page:\n",
    "        try:\n",
    "            all_page_id = page.find('a').get_text()\n",
    "            max_page_id = all_page_id\n",
    "        except:\n",
    "            pass\n",
    "    print (max_page_id)\n",
    "    print ('\\n')\n",
    "    \n",
    "    for i in range(1,int(max_page_id)+1):\n",
    "        if (i == 1):\n",
    "            soup_new = soup\n",
    "        else:\n",
    "            new_url = full_url.rstrip('/') + '-' + str(i)\n",
    "            print (new_url)\n",
    "            soup_new = try_request(new_url)\n",
    "        divs = soup_new.find_all('div', class_='tw-overflow-hidden tw-bg-white tw-rounded-8')\n",
    "        #print (divs)\n",
    "        for div in divs:\n",
    "            links = div.find_all('a')\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                text = link.get_text()\n",
    "                text = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', text).strip()\n",
    "                if href:\n",
    "                    print(f'{href}' + '\\t' + f'{text}')\n",
    "                    outdir = dir1 + '/' + text\n",
    "                    os.makedirs(outdir,exist_ok=True)\n",
    "                    exp_level2(href,outdir)\n",
    "                else:\n",
    "                    pass\n",
    "            time.sleep(2)\n",
    "            #break\n",
    "        time.sleep(2)\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_level1('/lab-web/exp/316nrk8go403k/',output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_url = root_url + 'lab-web/exp/'\n",
    "response = requests.get(basic_url)\n",
    "if response.status_code == 200:\n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    #print (response.text)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # 提取特定类名或 ID 的内容\n",
    "    # 例如，提取 class=\"example-class\" 的 <div> 标签的内容\n",
    "    divs = soup.find_all('div', class_='ant-tabs-tab')\n",
    "    for div in divs:\n",
    "        #print (div)\n",
    "        links = div.find_all('a')\n",
    "        #遍历所有 <a> 标签并提取 href 属性\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            text = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', text).strip()\n",
    "            if href:\n",
    "                print(f'{href}' + '\\t' + f'{text}')\n",
    "                outdir = output_dir + '/' + text\n",
    "                os.makedirs(outdir,exist_ok=True)\n",
    "                exp_level1(href,outdir) \n",
    "                time.sleep(1)\n",
    "            #break\n",
    "        time.sleep(5)\n",
    "        #break     \n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(new_url,dir1):\n",
    "        response_new = requests.get(new_url)\n",
    "        if response.status_code == 200:\n",
    "            soup_new = BeautifulSoup(response_new.text, 'html.parser')\n",
    "        else:\n",
    "             time.sleep(5)\n",
    "             soup_new = BeautifulSoup(response_new.text, 'html.parser')\n",
    "        divs = soup_new.find_all('div', class_='tw-overflow-hidden tw-bg-white tw-rounded-8')\n",
    "        #print (divs)\n",
    "        for div in divs:\n",
    "            links = div.find_all('a')\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                text = link.get_text()\n",
    "                text = re.sub(r'[^\\w\\u4e00-\\u9fff\\sa-zA-Z]', '', text).strip()\n",
    "                if href:\n",
    "                    print(f'{href}' + '\\t' + f'{text}')\n",
    "                    outdir = dir1 + '/' + text\n",
    "                    os.makedirs(outdir,exist_ok=True)\n",
    "                    exp_level2(href,outdir)\n",
    "                else:\n",
    "                    pass\n",
    "        print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_data = pd.read_csv('/public/home/liujunwu/workdir/MEPC/ExperimentMethod/rerun.list3',header=0,sep='\\t')\n",
    "#rerun_data['dir'] = rerun_data['dir'].apply(lambda x:'/public/home/liujunwu/workdir/MEPC/ExperimentMethod/'+str(x))\n",
    "print (rerun_data.shape)\n",
    "rerun_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in rerun_data.iterrows():\n",
    "    print (row['url'],row['dir'])\n",
    "    print (index)\n",
    "    rerun(row['url'],row['dir'])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 重新爬取空目录下的文档\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
